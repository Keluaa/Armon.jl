var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = Armon","category":"page"},{"location":"#Armon","page":"Home","title":"Armon","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Armon.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Armon is a 2D CFD solver for compressible non-viscous fluids, using the finite volume method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It was made to explore Julia's capabilities in HPC and for performance portability: it should perform very well on any CPU and GPU. Domain decomposition using MPI is supported.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The twin project Armon-Kokkos is a mirror of the core of this solver (with much less options) written in C++ using the Kokkos library. It is possible to reuse kernels from that solver in this one, using the Kokkos.jl package.","category":"page"},{"location":"#Parameters-and-entry-point","page":"Home","title":"Parameters and entry point","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ArmonParameters\narmon\nSolverStats\nStepsRanges\ndata_type\n@section","category":"page"},{"location":"#Armon.ArmonParameters","page":"Home","title":"Armon.ArmonParameters","text":"ArmonParameters(; options...)\n\nThe parameters and current state of the solver.\n\nThe state is reset at each call to armon.\n\nThere are many options. Each backend can add their own.\n\nOptions\n\nBackend and MPI\n\ndevice = :CUDA\n\nDevice to use. Supported values:\n\n:CPU_HP: Polyester.jl CPU multithreading  (default if use_gpu=false)\n:CUDA: CUDA.jl GPU (default if use_gpu=true)\n:ROCM: AMDGPU.jl GPU\n:CPU: KernelAbstractions.jl CPU multithreading (using the standard Threads.jl)\n\nuse_MPI = true, P = (1, 1), reorder_grid = true, global_comm = nothing\n\nMPI config. The MPI domain will be a process grid of size P. global_comm is the global communicator to use, defaults to MPI.COMM_WORLD. reorder_grid is passed to MPI.Cart_create.\n\ngpu_aware = true\n\nStore MPI buffers on the device. This requires to use a GPU-aware MPI implementation. Does nothing when using the CPU only.\n\nKernels\n\nuse_threading = true, use_simd = true\n\nSwitches for CPU_HP kernels. use_threading enables @threaded for outer loops. use_simd enables @simd_loop for inner loops.\n\nuse_gpu = false\n\nEnables the use of KernelAbstractions.jl kernels.\n\nuse_kokkos = false\n\nUse kernels for Kokkos.jl.\n\nuse_cache_blocking = true\n\nSeparate the domain into semi-independant blocks, improving the cache-locality of memory accesses and therefore memory throughput.\n\nblock_size = 1024\n\nSize of blocks for cache blocking. Can be a tuple. If use_cache_blocking == false, this option only controls the size of GPU blocks.\n\nuse_two_step_reduction = false\n\nReduction kernels (dtCFL_kernel and conservation_vars) use some optimizations to perform the reduction in a single step. It might cause issues on some GPU backends: a more \"gentle\" approach could avoid those by doing it in two steps.\n\nProfiling\n\nprofiling = Symbol[]\n\nList of profiling callbacks to use:\n\n:TimerOutputs: TimerOutputs.jl sections (added if measure_time=true)\n:NVTX_sections: NVTX.jl sections\n:NVTX_kernels: NVTX.jl sections for kernels\n:CUDA_kernels: equivalent to CUDA.@profile in front of all kernels\n\nmeasure_time = true\n\nmeasure_time=false can remove any overhead caused by profiling.\n\ntime_async = true\n\ntime_async=false will add a barrier at the end of every section. Useful for GPU kernels.\n\nScheme and CFD solver\n\nscheme = :GAD, riemann_limiter = :minmod\n\nscheme is the Riemann solver scheme to use:\n\n:Godunov (1st order)\n:GAD (2nd order, with limiter).\n\nriemann_limiter is the limiter to use for the Riemann solver: :no_limiter, :minmod or :superbee.\n\nprojection = :euler_2nd\n\nScheme for the Eulerian remap step:\n\n:euler (1st order)\n:euler_2nd (2nd order, +minmod limiter)\n\naxis_splitting = :Sequential\n\nAxis splitting to use:\n\n:Sequential: X then Y\n:SequentialSym: X and Y then Y and X, alternating\n:Strang: ½X, Y, ½X then ½Y, X, ½Y, alternating (½ is for halved time step)\n:X_only\n:Y_only\n\nN = (10, 10)\n\nNumber of cells of the global domain in each axes.\n\nnghost = 4\n\nNumber of ghost cells. Must be greater or equal to the minimum number of ghost cells (min 1, scheme=:GAD adds one, projection=:euler_2nd adds one, scheme=:GAD + projection=:euler_2nd adds another one)\n\nDt = 0., cst_dt = false, dt_on_even_cycles = false\n\nDt is the initial time step, it is computed after initialization by default. If cst_dt=true then the time step is always Dt and no reduction over the entire domain occurs.  If dt_on_even_cycles=true then then time step is only updated at even cycles (the first cycle is even).\n\ndata_type = Float64\n\nData type for all variables. Should be an AbstractFloat.\n\nTest case and domain\n\ntest = :Sod, domain_size = nothing, origin = nothing\n\ntest is the test case name to use:\n\n:Sod: Sod shock tube test\n:Sod_y: Sod shock tube test along the Y axis\n:Sod_circ: Circular Sod shock tube test (centered in the domain)\n:Bizarrium: Bizarrium test, similar to the Sod shock tube but with a special equation of state\n:Sedov: Sedov blast-wave test (centered in the domain, reaches the border at t=1 by default)\n:DebugIndexes: Set all variables to their index in the global domain. Debug only.\n\ncfl = 0., maxtime = 0., maxcycle = 500_000\n\ncfl defaults to the test's default value, same for maxtime. The solver stops when t reaches maxtime or maxcycle iterations were done (maxcycle=0 stops after initialization).\n\nOutput\n\nsilent = 0\n\nsilent=0 for maximum verbosity. silent=3 doesn't print info at each cycle. silent=5 doesn't print anything.\n\noutput_dir = \".\", output_file = \"output\"\n\njoinpath(output_dir, output_file) will be path to the output file.\n\nwrite_output = false, write_ghosts = false\n\nwrite_output=true will write all saved_vars() to the output file. If write_ghosts=true, ghost cells will also be included.\n\nwrite_slices = false\n\nWill write all saved_vars() to 3 output files, one for the middle X row, another for the middle Y column, and another for the diagonal. If write_ghosts=true, ghost cells will also be included.\n\noutput_precision = nothing\n\nNumbers are saved with output_precision digits of precision. Defaults to enough numbers for an exact decimal representation.\n\nanimation_step = 0\n\nIf animation_step ≥ 1, then every animation_step cycles, variables will be saved as with write_output=true.\n\ncompare = false, is_ref = false, comparison_tolerance = 1e-10\n\nIf compare=true, then at every sub step of each iteration of the solver all variables will:\n\n(is_ref=false) be compared with a reference file found in output_dir\n(is_ref=true) be saved to a reference file in output_dir\n\nWhen comparing, a relative comparison_tolerance (the rtol kwarg of isapprox) is accepted between values.\n\ncheck_result = false\n\nCheck if conservation of mass and energy is verified between initialization and the last iteration. An error is thrown otherwise. Accepts a relative comparison_tolerance.\n\nreturn_data = false\n\nIf return_data=true, then in the SolverStats returned by armon, the data field will contain the BlockGrid used by the solver.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.armon","page":"Home","title":"Armon.armon","text":"armon(::ArmonParameters)\n\nMain entry point of the solver. Returns a SolverStats.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.SolverStats","page":"Home","title":"Armon.SolverStats","text":"SolverStats\n\nSolver output.\n\ndata is nothing if parameters.return_data is false.\n\ntimer is nothing if parameters.measure_time is false.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.StepsRanges","page":"Home","title":"Armon.StepsRanges","text":"StepsRanges\n\nHolds indexing information for all steps of the solver.\n\nDomains are stored as block corner offsets: blocks can have different sizes, but always the same amount of ghost cells, therefore the iteration domain is determined from the dimensions of the block. The first field is the offset to the first cell, the second is the offset to the last cell.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.data_type","page":"Home","title":"Armon.data_type","text":"data_type(::ArmonParameters{T})\n\nGet T, the type used for numbers by the solver\n\n\n\n\n\n","category":"function"},{"location":"#Armon.@section","page":"Home","title":"Armon.@section","text":"@section(name, expr)\n@section(name, options, expr)\n\nIntroduce a profiling section around expr. Sections can be nested. Sections do not introduce a new scope.\n\nPlaced before a for-loop, a new section will be started for each iteration. name can interpolate using loop variables (like Test.@testset).\n\nIt is assumed that a params variable of ArmonParameters is present in the scope of the @section.\n\noptions is of the form key=value:\n\nasync (default: false): if async=false (and !params.time_async), a barrier (wait(params)) is added at the end of the section.\n\nparams = ArmonParameters(#= ... =#)\n\n@section \"Iteration $i\" for i in 1:10\n    j = @section \"Foo\" begin\n        foo(i)\n    end\n\n    @section \"Some calculation\" begin\n        k = bar(i, j)\n    end\n\n    @sync begin\n        @async begin\n            @section \"Task 1\" async=true my_task_1(i, j, k)\n        end\n\n        @async begin\n            @section \"Task 2\" async=true my_task_2(i, j, k)\n        end\n    end\nend\n\n\n\n\n\n","category":"macro"},{"location":"#Grid-and-blocks","page":"Home","title":"Grid and blocks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BlockGrid\ngrid_dimensions\nTaskBlock\nLocalTaskBlock\nRemoteTaskBlock\nBlockState\ndevice_to_host!\nhost_to_device!\nbuffers_on_device\ndevice_is_host\ndevice_blocks\nhost_blocks\nblock_idx\nedge_block_idx\nremote_block_idx\n@iter_blocks","category":"page"},{"location":"#Armon.BlockGrid","page":"Home","title":"Armon.BlockGrid","text":"BlockGrid{DeviceA, HostA, BufferA, Ghost, BlockSize, Device}\n\nStores TaskBlocks on the Device and host memory, in a grid.\n\nLocalTaskBlock are stored separately depending on if they have a StaticBSize of BlockSize (in device_blocks and host_blocks) or if they have a DynamicBSize (in device_edge_blocks or host_edge_blocks).\n\nBlocks have Ghost cells padding their real cells. This is included in their block_size. A block cannot have a number of real cells along an axis smaller than the number of ghost cells, unless there is only a single block in the grid along that axis.\n\nDeviceA and HostA are AbstractArray types for the device and host respectively. If DeviceA == HostA, then device_blocks === host_blocks.\n\nBufferA is the type of storage used for MPI buffers. MPI buffers are homogenous: they are either all on the host or all on the device.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.grid_dimensions","page":"Home","title":"Armon.grid_dimensions","text":"grid_dimensions(params::ArmonParameters)\ngrid_dimensions(block_size::NTuple{D, Int}, domain_size::NTuple{D, Int}, ghost::Int) where {D}\n\nReturns the dimensions of the grid in the form (grid_size, static_sized_grid, remainder_block_size) from the block_size (the size of blocks in the static_sized_grid), the domain_size (number of real cells) and the number of ghost cells, common to all blocks.\n\ngrid_size is the static_sized_grid including the edge blocks. Edge blocks along the axis d have a size of remainder_block_size[d] along d, and block_size for the other axes.\n\nblock_size includes the ghost cells in its dimensions, which must all be greater than 2*ghost.\n\nIf prod(block_size) == 0, then block_size is ignored and the grid is made of only a single block of size domain_size.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.TaskBlock","page":"Home","title":"Armon.TaskBlock","text":"TaskBlock{V}\n\nAbstract block used for cache blocking.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.LocalTaskBlock","page":"Home","title":"Armon.LocalTaskBlock","text":"LocalTaskBlock{V, Size} <: TaskBlock{V}\n\nContainer of Size and variables of type V, part of a BlockGrid. One block can run all solver steps independantly of all other blocks, apart from those which require updating ghost cells.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.RemoteTaskBlock","page":"Home","title":"Armon.RemoteTaskBlock","text":"RemoteTaskBlock{B} <: TaskBlock{B}\n\nBlock located at the border of a BlockGrid, containing MPI buffer of type B for communication with other BlockGrids.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.BlockState","page":"Home","title":"Armon.BlockState","text":"BlockState\n\nDone:    data is up-do-date with the current solver's state\nWorking: one thread is working on the block's data\nWaiting: waiting for the completion of another block(s) before continuing work\n\nAt any state, host and device memory are not guarenteed to be synced.\n\nTwo blocks Waiting for each other are able to exchange their neighbouring regions.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.device_to_host!","page":"Home","title":"Armon.device_to_host!","text":"device_to_host!(blk::LocalTaskBlock)\n\nCopies the device data of blk to its host mirror. blk can be the device or host block. A no-op if the device is the host.\n\n\n\n\n\ndevice_to_host!(grid::BlockGrid)\n\nCopies all device data to the host blocks. A no-op if the device is the host.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.host_to_device!","page":"Home","title":"Armon.host_to_device!","text":"device_to_host!(blk::LocalTaskBlock)\n\nCopies the host data of blk to its device mirror. blk can be the device or host block. A no-op if the device is the host.\n\n\n\n\n\ndevice_to_host!(grid::BlockGrid)\n\nCopies all host data to the device blocks. A no-op if the device is the host.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.buffers_on_device","page":"Home","title":"Armon.buffers_on_device","text":"buffers_on_device(::BlockGrid)\nbuffers_on_device(::Type{<:BlockGrid})\n\ntrue if the communication buffers are stored on the device, allowing direct transfers without passing through the host (GPU-aware communication).\n\n\n\n\n\n","category":"function"},{"location":"#Armon.device_is_host","page":"Home","title":"Armon.device_is_host","text":"device_is_host(::BlockGrid{D, H})\ndevice_is_host(::Type{<:BlockGrid{D, H}})\n\ntrue if the device is the host, i.e. device blocks and host blocks are the same (and D == H).\n\n\n\n\n\n","category":"function"},{"location":"#Armon.device_blocks","page":"Home","title":"Armon.device_blocks","text":"device_blocks(grid::BlockGrid)\n\nSimple iterator over all device blocks.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.host_blocks","page":"Home","title":"Armon.host_blocks","text":"host_blocks(grid::BlockGrid)\n\nSimple iterator over all host blocks.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_idx","page":"Home","title":"Armon.block_idx","text":"Linear index of a block in the statically-sized grid\n\n\n\n\n\n","category":"function"},{"location":"#Armon.edge_block_idx","page":"Home","title":"Armon.edge_block_idx","text":"Linear index of a block at the (dynamically-sized) edges of the grid\n\n\n\n\n\n","category":"function"},{"location":"#Armon.remote_block_idx","page":"Home","title":"Armon.remote_block_idx","text":"Linear index of a remote block at the edges of the grid\n\n\n\n\n\n","category":"function"},{"location":"#Armon.@iter_blocks","page":"Home","title":"Armon.@iter_blocks","text":"@iter_blocks for blk in device_blocks(grid)\n    # body...\nend\n\nApplies the body of the for-loop in to all blocks of the grid.\n\nThe body is duplicated for inner and edge blocks, ensuring type-inference.\n\nIf params.use_multithreading, then an attempt will be made at equilibrating workload among threads.\n\n# Iterate on all device blocks\n@iter_blocks for blk in device_blocks(grid)\n    some_function(blk)\nend\n\n# Iterate on all host blocks\n@iter_blocks for blk in host_blocks(grid)\n    some_function(blk)\nend\n\n\n\n\n\n","category":"macro"},{"location":"#Block-size-and-iteration","page":"Home","title":"Block size and iteration","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BlockSize\nStaticBSize\nDynamicBSize\nblock_size\nreal_block_size\nghosts\nborder_domain\nghost_domain\nblock_domain_range\nposition\nlin_position\nin_grid\nis_ghost\nBlockRowIterator\nDomainRange","category":"page"},{"location":"#Armon.BlockSize","page":"Home","title":"Armon.BlockSize","text":"BlockSize\n\nDimensions of a LocalTaskBlock.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.StaticBSize","page":"Home","title":"Armon.StaticBSize","text":"StaticBSize{S, Ghost} <: BlockSize\n\nA BlockSize of size S and Ghost cells. S is embedded in the type, this reduces the amount of memory in the parameters of every kernel, as well as allowing the compiler to make some minor optizations in indexing expressions.\n\nGhost cells are included in S: there are S .- 2*Ghost real cells.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.DynamicBSize","page":"Home","title":"Armon.DynamicBSize","text":"DynamicBSize{Ghost} <: BlockSize\n\nSimilar to StaticBSize, but for blocks with a less-than-ideal size: block size is therefore not stored in the type. This results in less compilation when testing for different domain sizes with a constant StaticBSize.\n\nThe number of Ghost cells is still embedded in the type, as it can simplify some indexing expressions and for coherency.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.block_size","page":"Home","title":"Armon.block_size","text":"NTuple of the dimensions of a BlockSize\n\n\n\n\n\n","category":"function"},{"location":"#Armon.real_block_size","page":"Home","title":"Armon.real_block_size","text":"NTuple of the dimensions of a BlockSize, excluding ghost cells\n\n\n\n\n\n","category":"function"},{"location":"#Armon.ghosts","page":"Home","title":"Armon.ghosts","text":"Number of ghost cells of a BlockSize\n\n\n\n\n\n","category":"function"},{"location":"#Armon.border_domain","page":"Home","title":"Armon.border_domain","text":"border_domain(bsize::BlockSize, side::Side)\n\nDomainRange of the real cells along side. It includes only one \"strip\" of cells, that is length(border_domain(bsize, side)) == size_along(bsize, side).\n\n\n\n\n\n","category":"function"},{"location":"#Armon.ghost_domain","page":"Home","title":"Armon.ghost_domain","text":"ghost_domain(params::ArmonParameters, side::Side)\n\nDomainRange of all ghosts cells of side, excluding the corners of the block.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_domain_range","page":"Home","title":"Armon.block_domain_range","text":"block_domain_range(bsize::BlockSize, corners)\nblock_domain_range(bsize::BlockSize, bottom_left::Tuple, top_right::Tuple)\n\nA DomainRange built from offsets from the corners of bsize.\n\nblock_domain_range(bsize, (0, 0), (0, 0)) is the domain of all real cells in the block. block_domain_range(bsize, (-g, -g), (g, g)) would be the domain of all cells (real cells + g ghost cells) in the block.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.position","page":"Home","title":"Armon.position","text":"position(bsize::BlockSize, i)\n\nN-dim position of the i-th cell in the block.\n\nIf 1 ≤ position(bsize, i)[d] ≤ block_size(bsize)[d] then the cell is not a ghost cell along the d dimension. See is_ghost.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.lin_position","page":"Home","title":"Armon.lin_position","text":"lin_position(bsize::BlockSize, I)\n\nFrom the NTuple (e.g. returned from position), return the linear index in the block. lin_position(bsize, position(bsize, i)) == i.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.in_grid","page":"Home","title":"Armon.in_grid","text":"in_grid(idx, size)\nin_grid(start, idx, size)\n\ntrue if each axis of idx is between start and size. start defaults to 1.\n\nArgument types can any mix of Integer, Tuple or CartesianIndex.\n\njulia> in_grid(1, (1, 2), 2)  # same as `in_grid((1, 1), (1, 2), (2, 2))`\ntrue\n\njulia> in_grid((3, 1), (3, 2))\ntrue\n\njulia> in_grid((1, 3), (3, 2))\nfalse\n\n\n\n\n\nin_grid(idx, grid, axis::Axis)\nin_grid(start, idx, grid, axis::Axis)\n\nSame as in_grid(start, idx, grid), but only checks along axis.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.is_ghost","page":"Home","title":"Armon.is_ghost","text":"is_ghost(bsize::BlockSize, i, o=0)\n\ntrue if the i-th cell of the block is a ghost cell, false otherwise.\n\no would be a \"ring\" index: o == 1 excludes the first ring of ghost cells, etc.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.BlockRowIterator","page":"Home","title":"Armon.BlockRowIterator","text":"BlockRowIterator(grid::BlockGrid; kwargs...)\nBlockRowIterator(grid::BlockGrid, blk::LocalTaskBlock; kwargs...)\nBlockRowIterator(grid::BlockGrid, sub_grid; global_ghosts=false, all_ghosts=false, device_blocks=true)\n\nIterate the rows of all blocks of the grid, row by row (and not block by block). This allows to iterate the cells of the grid as if it was a single block.\n\nGiving blk will return an iterator on the rows of the block (device_blocks is deduced).\n\nsub_grid defaults to the whole grid: it is a Tuple of iterables, one for each axis.\n\nIf global_ghosts == true, then the ghost cells of at the border of the global domain are also returned. If all_ghosts == true, then the ghost cells of at the border of all blocks are also returned. If device_blocks == false, then host blocks are returned instead of device blocks.\n\njulia> for (blk, row_range) in BlockRowIterator(grid; all_ghosts=true)\n           println(blk.pos, \" - \", row_range)\n       end\n(1, 1) - 1:64\n(2, 1) - 1:64\n(1, 1) - 65:128\n(2, 1) - 65:128\n(1, 1) - 129:192\n(2, 1) - 129:192\n...\n\n\n\n\n\n","category":"type"},{"location":"#Armon.DomainRange","page":"Home","title":"Armon.DomainRange","text":"DomainRange\n\nTwo dimensional range to index a 2D array stored with contiguous rows. Not equivalent to CartesianIndices as it handles StepRanges properly.\n\n\n\n\n\n","category":"type"},{"location":"#Device-and-backends","page":"Home","title":"Device and backends","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CPU_HP\ncreate_device\ninit_backend\ndevice_memory_info\nmemory_info\nmemory_required","category":"page"},{"location":"#Armon.CPU_HP","page":"Home","title":"Armon.CPU_HP","text":"CPU_HP\n\nDevice tag for the high-performance CPU backend using multithreading (with Polyester.jl) and vectorisation.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.create_device","page":"Home","title":"Armon.create_device","text":"create_device(::Val{:device_name})\n\nCreate a device object from its name.\n\nDefault devices:\n\n:CPU: the CPU backend of KernelAbstractions.jl\n:CPU_HP: Polyester.jl multithreading\n\nExtensions:\n\n:Kokkos: the default Kokkos.jl device\n:CUDA: the CUDA.jl backend of KernelAbstractions.jl\n:ROCM: the AMDGPU.jl backend of KernelAbstractions.jl\n\n\n\n\n\n","category":"function"},{"location":"#Armon.init_backend","page":"Home","title":"Armon.init_backend","text":"init_backend(params::ArmonParameters, ::Dev; options...)\n\nInitialize the backend corresponding to the Dev device returned by create_device using options. Set the params.backend_options field.\n\nIt must return options, with the backend-specific options removed.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.device_memory_info","page":"Home","title":"Armon.device_memory_info","text":"device_memory_info(device)\n\nThe total and free memory on the device, in bytes.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.memory_info","page":"Home","title":"Armon.memory_info","text":"memory_info(params)\n\nThe total and free memory the current process can store on the params.device.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.memory_required","page":"Home","title":"Armon.memory_required","text":"memory_required(params::ArmonParameters)\n\n(device_memory, host_memory) required for params.\n\nMPI buffers are included in the appropriate field depending on params.gpu_aware.\n\nIf device_is_host, then, device_memory only includes memory required by data arrays and MPI buffers.\n\n\n\n\n\nmemory_required(N::NTuple{2, Int}, block_size::NTuple{2, Int}, ghost::Int, data_type)\nmemory_required(N::NTuple{2, Int}, block_size::NTuple{2, Int}, ghost::Int, DeviceArray, HostArray, BufferArray)\n\nCompute the number of bytes needed to allocate all blocks. If only data_type is given, then DeviceArray, HostArray and BufferArray default to Vector{T}.\n\nIn order of returned values:\n\nAmount of bytes needed for all arrays on the device. This amount is also required on the host when the host and device are not the same.\nAmount of bytes needed for all MPI buffers, if the sub-domain has neighbours on all of its sides. If params.gpu_aware, then this memory is allocated on the device.\nAmount of bytes needed on the host memory for all block objects, excluding array data and buffers. This memory is always allocated on the host, and includes device and host blocks objects if DeviceArray != HostArray.\n\nres = memory_required((1000, 1000), (64, 64), 4, CuArray{Float64}, Vector{Float64}, Vector{Float64})\ndevice_memory = res[1] + (params.gpu_aware ? res[2] : 0)\nhost_memory = res[3] + (device_is_host ? device_memory : res[1])\n\n\n\n\n\n","category":"function"},{"location":"#Kernels","page":"Home","title":"Kernels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"@generic_kernel\n@kernel_init\n@kernel_options\n@index_1D_lin\n@index_2D_lin\n@iter_idx\n@simd_loop\n@simd_threaded_iter\n@simd_threaded_loop\n@threaded\n@threads","category":"page"},{"location":"#Armon.@generic_kernel","page":"Home","title":"Armon.@generic_kernel","text":"@generic_kernel(function definition)\n\nTransforms a single kernel function into six different functions:\n\n4 which run on the CPU using Polyester.jl's multi-threading or not, as well as SIMD or not.\none which uses KernelAbstractions.jl to make a GPU-compatible kernel\na main function, which will take care of calling the two others depending if we want to use the GPU or not.\n\nTo do this, two things are done:\n\nAll calls to @index_1D_lin(), @index_2D_lin() and @iter_idx() are replaced by their equivalent in their respective platforms: a simple loop index for CPUs, and a call to KA.jl's @index for GPUs.\nArguments to each function are edited \n\nA kernel function must call one of @index_1D_lin() or @index_2D_lin() at least once, since this  will determine which type of indexing to use as well as which parameters to add.\n\nThe indexing macro @iter_idx gives the linear index to the current iteration (on CPU) or global  thread (on GPU).\n\nThis means that in order to call the new main function, one needs to take into account which indexing macro was used:\n\nIn all cases, params::ArmonParameters is the first argument\nThen, depending on the indexing macro used:\n@index_1D_lin() : loop_range::OrdinalRange{Int}\n@index_2D_lin() : main_range::OrdinalRange{Int}, inner_range::OrdinalRange{Int}\nAn optional keyword argument, no_threading, allows to override the use_threading parameter, which can be useful in asynchronous contexts. It defaults to false.\n\nUsing KA.jl's @Const to annotate arguments is supported, but they will be present only in the GPU kernel definition.\n\nFurther customisation of the kernel and main function can be obtained using @kernel_options and @kernel_init.\n\n@generic_kernel f_kernel(A)\n    i = @index_1D_lin()\n    A[i] += 1\nend\n\nparams.use_gpu = false\nf(params, 1:10)  # CPU call\n\nparams.use_gpu = true\nf(params, 1:10)  # GPU call\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@kernel_init","page":"Home","title":"Armon.@kernel_init","text":"@kernel_init(expr)\n\nAllows to initialize some internal variables of the kernel before the loop. The given expression must NOT depend on any index.  You must not use any indexing macro (@index_1D_lin(), etc...) in the expression.\n\nAll paramerters of the kernel are available during the execution of the init expression. On GPU, this expression will be executed by each thread.\n\nThis is a workaround for a limitation of Polyester.jl which prevents you from typing variables.\n\n@generic_kernel function simple_kernel(a, b)\n    @kernel_init begin\n        c::Float64 = sin(0.123)\n    end\n    i = @index_1D_lin()\n    a[i] += b[i] * c\nend\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@kernel_options","page":"Home","title":"Armon.@kernel_options","text":"@kernel_options(options...)\n\nMust be used (once, and explicitly) in the definition of a @generic_kernel function.\n\nGives options for @generic_kernel to adjust the resulting functions.\n\nThe possible options are:\n\ndebug:      Prints the generated functions to stdout at compile time.\n\n@generic_kernel function add_kernel(a, b)\n    @kernel_options(debug)\n    i = @index_1D_lin()\n    a[i] += b[i]\nend\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@index_1D_lin","page":"Home","title":"Armon.@index_1D_lin","text":"@index_1D_lin()\n\nIndexing macro to use in a @generic_kernel function.  Returns a linear index to access the 1D arrays used by the kernel.\n\nCannot be used alongside @index_2D_lin().\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@index_2D_lin","page":"Home","title":"Armon.@index_2D_lin","text":"@index_2D_lin()\n\nIndexing macro to use in a @generic_kernel function.  Returns a linear index to access the 2D arrays used by the kernel.\n\nCannot be used alongside @index_1D_lin().\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@iter_idx","page":"Home","title":"Armon.@iter_idx","text":"@iter_idx()\n\nIndexing macro to use in a @generic_kernel function.  Returns a linear index to access the 2D arrays used by the kernel.\n\nEquivalent to KernelAbstractions.jl's @index(Global, Linear).\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@simd_loop","page":"Home","title":"Armon.@simd_loop","text":"@simd_loop(expr)\n\nAllows to enable/disable SIMD optimisations for a loop. When SIMD is enabled, it is assumed that there is no dependencies between each iterations of the loop.\n\n    @simd_loop for i = 1:n\n        y[i] = x[i] * (x[i-1])\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@simd_threaded_iter","page":"Home","title":"Armon.@simd_threaded_iter","text":"@simd_threaded_iter(range, expr)\n\nSame as @simd_threaded_loop(expr), but instead of slicing the range of the for loop in expr, we slice the range given as the first parameter and distribute the slices evenly to the threads.\n\nThe inner @simd loop assumes there is no dependencies between each iteration.\n\n    @simd_threaded_iter 4:2:100 for i in 1:100\n        y[i] = log10(x[i]) + x[i]\n    end\n    # is equivalent to (without threading and SIMD)\n    for j in 4:2:100\n        for i in (1:100) .+ (j - 1)\n            y[i] = log10(x[i]) + x[i]\n        end\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@simd_threaded_loop","page":"Home","title":"Armon.@simd_threaded_loop","text":"@simd_threaded_loop(expr)\n\nAllows to enable/disable multithreading and/or SIMD of the loop depending on the parameters. When using SIMD, @fastmath and @inbounds are used.\n\nIn order to use SIMD and multithreading at the same time, the range of the loop is split in even  batches. Each batch has a size of params.simd_batch iterations, meaning that the inner @simd loop has a fixed number of iterations, while the outer threaded loop will have N ÷ params.simd_batch iterations.\n\nThe loop range is assumed to be increasing, i.e. this is correct: 1:2:100, this is not: 100:-2:1 The inner @simd loop assumes there is no dependencies between each iteration.\n\n    @simd_threaded_loop for i = 1:n\n        y[i] = log10(x[i]) + x[i]\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@threaded","page":"Home","title":"Armon.@threaded","text":"@threaded(expr)\n\nAllows to enable/disable multithreading of the loop depending on the parameters.\n\n    @threaded for i = 1:n\n        y[i] = log10(x[i]) + x[i]\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@threads","page":"Home","title":"Armon.@threads","text":"Controls which multi-threading library to use.\n\n\n\n\n\n","category":"macro"},{"location":"#Utility","page":"Home","title":"Utility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Axis\nSide\nSolverException","category":"page"},{"location":"#Armon.Axis","page":"Home","title":"Armon.Axis","text":"Axis\n\nEnumeration of the axes of the domain\n\n\n\n\n\n","category":"type"},{"location":"#Armon.Side","page":"Home","title":"Armon.Side","text":"Side\n\nEnumeration of the sides of the domain\n\n\n\n\n\n","category":"type"},{"location":"#Armon.SolverException","page":"Home","title":"Armon.SolverException","text":"SolverException\n\nThrown when the solver encounters an invalid state.\n\nThe category field can be used to distinguish between error types without inspecting the error message:\n\n:config: a problem in the solver configuration, usually thrown when constructing ArmonParameters\n:cpp: a C++ exception thrown by the C++ Kokkos backend\n:time: an invalid time step\n\nErrorExceptions thrown by the solver represent internal errors.\n\n\n\n\n\n","category":"type"}]
}
