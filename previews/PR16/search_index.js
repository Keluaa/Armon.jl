var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = Armon","category":"page"},{"location":"#Armon","page":"Home","title":"Armon","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Armon.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Armon is a 2D CFD solver for compressible non-viscous fluids, using the finite volume method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It was made to explore Julia's capabilities in HPC and for performance portability: it should perform very well on any CPU and GPU. Domain decomposition using MPI is supported.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The twin project Armon-Kokkos is a mirror of the core of this solver (with much less options) written in C++ using the Kokkos library. It is possible to reuse kernels from that solver in this one, using the Kokkos.jl package.","category":"page"},{"location":"#Parameters-and-entry-point","page":"Home","title":"Parameters and entry point","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ArmonParameters\narmon\nSolverStats\nStepsRanges\ndata_type","category":"page"},{"location":"#Armon.ArmonParameters","page":"Home","title":"Armon.ArmonParameters","text":"ArmonParameters(; options...)\n\nThe parameters and current state of the solver.\n\nThe state is reset at each call to armon.\n\nThere are many options. Each backend can add their own.\n\nOptions\n\nBackend and MPI\n\ndevice = :CUDA\n\nDevice to use. Supported values:\n\n:CPU_HP: Polyester.jl CPU multithreading  (default if use_gpu=false)\n:CUDA: CUDA.jl GPU (default if use_gpu=true)\n:ROCM: AMDGPU.jl GPU\n:CPU: KernelAbstractions.jl CPU multithreading (using the standard Threads.jl)\n\nuse_MPI = true, P = (1, 1), reorder_grid = true, global_comm = nothing\n\nMPI config. The MPI domain will be a process grid of size P. global_comm is the global communicator to use, defaults to MPI.COMM_WORLD. reorder_grid is passed to MPI.Cart_create.\n\ngpu_aware = true\n\nStore MPI buffers on the device. This requires to use a GPU-aware MPI implementation. Does nothing when using the CPU only.\n\nnuma_aware = true\n\nAllocate memory according to which NUMA node is associated with the thread supposed to work on a chunk of memory. This effectively enforces the first-touch policy, instead of blindly relying on it.\n\nlock_memory = false\n\nLock all memory pages using mlock to RAM.\n\nKernels\n\nuse_threading = true, use_simd = true\n\nSwitches for CPU_HP kernels. use_threading enables @threaded for outer loops. use_simd enables @simd_loop for inner loops.\n\nuse_gpu = false\n\nEnables the use of KernelAbstractions.jl kernels.\n\nuse_kokkos = false\n\nUse kernels for Kokkos.jl.\n\nuse_cache_blocking = true\n\nSeparate the domain into semi-independant blocks, improving the cache-locality of memory accesses and therefore memory throughput.\n\nasync_cycle = false\n\nApply all steps of the solver to all blocks asynchronously, fully taking advantage of cache blocking.\n\nblock_size = 1024\n\nSize of blocks for cache blocking. Can be a tuple. If use_cache_blocking == false, this option only controls the size of GPU blocks.\n\nuse_two_step_reduction = false\n\nReduction kernels (dtCFL_kernel and conservation_vars) use some optimizations to perform the reduction in a single step. It might cause issues on some GPU backends: a more \"gentle\" approach could avoid those by doing it in two steps.\n\nworkload_distribution = :simple\n\nDictates how blocks are distributed among threads when async_cycle == true:\n\n:simple trivially spreads all blocks to all threads evenly\n:square divides the block grid into rectangles (closely shaped as squares) for each thread, with the goal of minimizing the perimeter of rectangles to reduce the interaction with other threads as much as possible\n:sorted_square is the same as :square, but additionally sorts the blocks to work on those at the perimeter of the square first, reducing the likelyness of waiting for neighbouring threads\n:scotch uses the Scotch solver to partition the block grid\n:sorted_scotch is the same as :scotch, but with the same additional sorting strategy of :sorted_square\n:weighted_sorted_scotch takes into account the number of cells in each block instead of assuming an even workload for all blocks\n\nProfiling\n\nprofiling = Symbol[]\n\nList of profiling callbacks to use:\n\n:TimerOutputs: TimerOutputs.jl sections (added if measure_time=true)\n:NVTX_sections: NVTX.jl sections\n:NVTX_kernels: NVTX.jl sections for kernels\n:CUDA_kernels: equivalent to CUDA.@profile in front of all kernels\n\nmeasure_time = true\n\nmeasure_time=false can remove any overhead caused by profiling.\n\ntime_async = true\n\ntime_async=false will add a barrier at the end of every section. Useful for GPU kernels.\n\nScheme and CFD solver\n\nscheme = :GAD, riemann_limiter = :minmod\n\nscheme is the Riemann solver scheme to use:\n\n:Godunov (1st order)\n:GAD (2nd order, with limiter).\n\nriemann_limiter is the limiter to use for the Riemann solver: :no_limiter, :minmod or :superbee.\n\nprojection = :euler_2nd\n\nScheme for the Eulerian remap step:\n\n:euler (1st order)\n:euler_2nd (2nd order, +minmod limiter)\n\naxis_splitting = :Sequential\n\nAxis splitting to use:\n\n:Sequential: X then Y\n:SequentialSym (or :Godunov): X and Y then Y and X, alternating\n:Strang: ½X, Y, ½X then ½Y, X, ½Y, alternating (½ is for halved time step)\n:X_only\n:Y_only\n\nN = (10, 10)\n\nNumber of cells of the global domain in each axes.\n\nnghost = 4\n\nNumber of ghost cells. Must be greater or equal to the minimum number of ghost cells (min 1, scheme=:GAD adds one, projection=:euler_2nd adds one, scheme=:GAD + projection=:euler_2nd adds another one)\n\nDt = 0., cst_dt = false, dt_on_even_cycles = false\n\nDt is the initial time step, it is computed after initialization by default. If cst_dt=true then the time step is always Dt and no reduction over the entire domain occurs.  If dt_on_even_cycles=true then then time step is only updated at even cycles (the first cycle is even).\n\ndata_type = Float64\n\nData type for all variables. Should be an AbstractFloat.\n\nTest case and domain\n\ntest = :Sod, domain_size = nothing, origin = nothing\n\ntest is the test case name to use:\n\n:Sod: Sod shock tube test\n:Sod_y: Sod shock tube test along the Y axis\n:Sod_circ: Circular Sod shock tube test (centered in the domain)\n:Bizarrium: Bizarrium test, similar to the Sod shock tube but with a special equation of state\n:Sedov: Sedov blast-wave test (centered in the domain, reaches the border at t=1 by default)\n:DebugIndexes: Set all variables to their index in the global domain. Debug only.\n\ncfl = 0., maxtime = 0., maxcycle = 500_000\n\ncfl defaults to the test's default value, same for maxtime. The solver stops when t reaches maxtime or maxcycle iterations were done (maxcycle=0 stops after initialization).\n\nOutput\n\nsilent = 0\n\nsilent=0 for maximum verbosity. silent=3 doesn't print info at each cycle. silent=5 doesn't print anything.\n\noutput_dir = \".\", output_file = \"output\"\n\njoinpath(output_dir, output_file) will be path to the output file.\n\nwrite_output = false, write_ghosts = false\n\nwrite_output=true will write all saved_vars() to the output file. If write_ghosts=true, ghost cells will also be included.\n\nwrite_slices = false\n\nWill write all saved_vars() to 3 output files, one for the middle X row, another for the middle Y column, and another for the diagonal. If write_ghosts=true, ghost cells will also be included.\n\noutput_precision = nothing\n\nNumbers are saved with output_precision digits of precision. Defaults to enough numbers for an exact decimal representation.\n\nanimation_step = 0\n\nIf animation_step ≥ 1, then every animation_step cycles, variables will be saved as with write_output=true.\n\ncompare = false, is_ref = false, comparison_tolerance = 1e-10\n\nIf compare=true, then at every sub step of each iteration of the solver all variables will:\n\n(is_ref=false) be compared with a reference file found in output_dir\n(is_ref=true) be saved to a reference file in output_dir\n\nWhen comparing, a relative comparison_tolerance (the rtol kwarg of isapprox) is accepted between values.\n\ncheck_result = false\n\nCheck if conservation of mass and energy is verified between initialization and the last iteration. An error is thrown otherwise. Accepts a relative comparison_tolerance.\n\nreturn_data = false\n\nIf return_data=true, then in the SolverStats returned by armon, the data field will contain the BlockGrid used by the solver.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.armon","page":"Home","title":"Armon.armon","text":"armon(::ArmonParameters)\n\nMain entry point of the solver. Returns a SolverStats.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.SolverStats","page":"Home","title":"Armon.SolverStats","text":"SolverStats\n\nSolver output.\n\ndata is nothing if parameters.return_data is false.\n\ntimer is nothing if parameters.measure_time is false.\n\ngrid_log is nothing if parameters.log_blocks is false.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.StepsRanges","page":"Home","title":"Armon.StepsRanges","text":"StepsRanges\n\nHolds indexing information for all steps of the solver.\n\nDomains are stored as block corner offsets: blocks can have different sizes, but always the same amount of ghost cells, therefore the iteration domain is determined from the dimensions of the block. The first field is the offset to the first cell, the second is the offset to the last cell.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.data_type","page":"Home","title":"Armon.data_type","text":"data_type(::ArmonParameters{T})\n\nGet T, the type used for numbers by the solver\n\n\n\n\n\n","category":"function"},{"location":"#Grid-and-blocks","page":"Home","title":"Grid and blocks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BlockGrid\ngrid_dimensions\nTaskBlock\nLocalTaskBlock\nBlockData\nRemoteTaskBlock\ndevice_to_host!\nhost_to_device!\nbuffers_on_device\ndevice_is_host\nall_blocks\nblock_idx\nedge_block_idx\nremote_block_idx\nEdgeBlockRegions\nRemoteBlockRegions\n@iter_blocks\nblock_pos_containing_cell\nblock_origin\nblock_at\nblock_size_at\nmove_pages(::BlockGrid)\nlock_pages(::BlockGrid)","category":"page"},{"location":"#Armon.BlockGrid","page":"Home","title":"Armon.BlockGrid","text":"BlockGrid{T, DeviceA, HostA, BufferA, Ghost, BlockSize, Device, SolverState}\n\nStores TaskBlocks on the Device and host memory, in a grid.\n\nLocalTaskBlock are stored separately depending on if they have a StaticBSize of BlockSize (in blocks) or if they have a DynamicBSize (in edge_blocks).\n\nBlocks have Ghost cells padding their real cells. This is included in their block_size. A block cannot have a number of real cells along an axis smaller than the number of ghost cells, unless there is only a single block in the grid along that axis.\n\n\"Edge blocks\" are blocks located on the right and/or top edge of the grid. They exist in order to handle domains with dimensions which are not multiples of the block size.\n\nDeviceA and HostA are AbstractArray types for the device and host respectively.\n\nBufferA is the type of storage used for MPI buffers. MPI buffers are homogenous: they are either all on the host or all on the device.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.grid_dimensions","page":"Home","title":"Armon.grid_dimensions","text":"grid_dimensions(params::ArmonParameters)\ngrid_dimensions(block_size::NTuple{D, Int}, domain_size::NTuple{D, Int}, ghost::Int) where {D}\n\nReturns the dimensions of the grid in the form (grid_size, static_sized_grid, remainder_block_size) from the block_size (the size of blocks in the static_sized_grid), the domain_size (number of real cells) and the number of ghost cells, common to all blocks.\n\ngrid_size is the static_sized_grid including the edge blocks. Edge blocks along the axis d have a size of remainder_block_size[d] along d only if they are at the edge of the grid along d, or block_size otherwise.\n\nblock_size includes the ghost cells in its dimensions, which must all be greater than 2*ghost.\n\nIf prod(block_size) == 0, then block_size is ignored and the grid is made of only a single block of size domain_size.\n\nnote: Note\nBlocks must not be smaller than ghost, therefore edge blocks might be made bigger than block_size.\n\nnote: Note\nIn case domain_size is smaller than block_size .- 2*ghost along any axis, the grid will contain only edge blocks.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.TaskBlock","page":"Home","title":"Armon.TaskBlock","text":"TaskBlock{V}\n\nAbstract block used for cache blocking.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.LocalTaskBlock","page":"Home","title":"Armon.LocalTaskBlock","text":"LocalTaskBlock{D, H, Size, SolverState} <: TaskBlock{V}\n\nContainer of Size and variables of type D on the device and H on the host. Part of a BlockGrid.\n\nThe block stores its own solver state, allowing it to run all solver steps independantly of all other blocks, apart from steps requiring synchronization.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.BlockData","page":"Home","title":"Armon.BlockData","text":"BlockData{V}\n\nHolds the variables of all cells of a LocalTaskBlock.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.RemoteTaskBlock","page":"Home","title":"Armon.RemoteTaskBlock","text":"RemoteTaskBlock{B} <: TaskBlock{B}\n\nBlock located at the border of a BlockGrid, containing MPI buffer of type B for communication with other BlockGrids.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.device_to_host!","page":"Home","title":"Armon.device_to_host!","text":"device_to_host!(blk::LocalTaskBlock)\n\nCopies the device data of blk to the host data. A no-op if the device is the host.\n\n\n\n\n\ndevice_to_host!(grid::BlockGrid)\n\nCopies device data of all blocks to the host data. A no-op if the device is the host.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.host_to_device!","page":"Home","title":"Armon.host_to_device!","text":"device_to_host!(blk::LocalTaskBlock)\n\nCopies the host data of blk to its device data. A no-op if the device is the host.\n\n\n\n\n\ndevice_to_host!(grid::BlockGrid)\n\nCopies host data of all blocks to the device data. A no-op if the device is the host.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.buffers_on_device","page":"Home","title":"Armon.buffers_on_device","text":"buffers_on_device(::BlockGrid)\nbuffers_on_device(::Type{<:BlockGrid})\n\ntrue if the communication buffers are stored on the device, allowing direct transfers without passing through the host (GPU-aware communication).\n\n\n\n\n\n","category":"function"},{"location":"#Armon.device_is_host","page":"Home","title":"Armon.device_is_host","text":"device_is_host(::BlockGrid{T, D, H})\ndevice_is_host(::Type{<:BlockGrid{T, D, H}})\n\ntrue if the device is the host, i.e. device blocks and host blocks are the same (and D == H).\n\n\n\n\n\n","category":"function"},{"location":"#Armon.all_blocks","page":"Home","title":"Armon.all_blocks","text":"all_blocks(grid::BlockGrid)\n\nSimple iterator over all blocks of the grid, excluding RemoteTaskBlocks.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_idx","page":"Home","title":"Armon.block_idx","text":"block_idx(grid::BlockGrid, idx::CartesianIndex)\n\nLinear index in grid.blocks of the block at idx in the statically-sized grid.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.edge_block_idx","page":"Home","title":"Armon.edge_block_idx","text":"edge_block_idx(grid::BlockGrid, idx::CartesianIndex)\n\nLinear index in grid.edge_blocks of the block at idx, along the (dynamically-sized) edges of the grid.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.remote_block_idx","page":"Home","title":"Armon.remote_block_idx","text":"remote_block_idx(grid::BlockGrid, idx::CartesianIndex)\n\nLinear index in grid.remote_blocks of the remote block at idx in the grid.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.EdgeBlockRegions","page":"Home","title":"Armon.EdgeBlockRegions","text":"EdgeBlockRegions(grid::BlockGrid; real_sizes=false)\nEdgeBlockRegions(\n    grid_size::NTuple{D, Int}, static_sized_grid::NTuple{D, Int},\n    block_size::NTuple{D, Int}, remainder_block_size::NTuple{D, Int}; ghosts=0\n)\n\nIterator over edge blocks, their positions and their size in each edge region of the grid.\n\nghosts only affects the size of edge blocks: B .- 2*ghosts. If real_sizes == true, then ghosts is ghosts(grid), therefore the real block size of edge blocks is returned.\n\nThere is a maximum of 2^D-1 edge regions in a grid (see this explanation). When grid_size[i] != static_sized_grid[i], there cannot be an edge region, hence there are exactly 2^sum(grid_size .!= static_sized_grid)-1 regions.\n\njulia> collect(Armon.EdgeBlockRegions((5, 5), (4, 4), (32, 32), (16, 16)))\n3-element Vector{Tuple{Int64, CartesianIndices{2, Tuple{UnitRange{Int64}, UnitRange{Int64}}}, Tuple{Int64, Int64}}}:\n (4, CartesianIndices((5:5, 1:4)), (16, 32))\n (4, CartesianIndices((1:4, 5:5)), (32, 16))\n (1, CartesianIndices((5:5, 5:5)), (16, 16))\n\njulia> collect(Armon.EdgeBlockRegions((5, 5, 5), (4, 5, 4), (32, 32, 32), (16, 16, 16)))\n3-element Vector{Tuple{Int64, CartesianIndices{3, Tuple{UnitRange{Int64}, UnitRange{Int64}, UnitRange{Int64}}}, Tuple{Int64, Int64, Int64}}}:\n (20, CartesianIndices((5:5, 1:5, 1:4)), (16, 32, 32))\n (20, CartesianIndices((1:4, 1:5, 5:5)), (32, 32, 16))\n (5, CartesianIndices((5:5, 1:5, 5:5)), (16, 32, 16))\n\n\n\n\n\n","category":"type"},{"location":"#Armon.RemoteBlockRegions","page":"Home","title":"Armon.RemoteBlockRegions","text":"RemoteBlockRegions(grid::BlockGrid)\nRemoteBlockRegions(grid_size::NTuple{D, Int})\n\nIterator of all remote block positions in each region (the faces of the grid). There is always 2*D regions.\n\njulia> collect(Armon.RemoteBlockRegions((5, 3)))\n4-element Vector{Tuple{Armon.Side.T, CartesianIndices{2, Tuple{UnitRange{Int64}, UnitRange{Int64}}}}}:\n (Armon.Side.Left, CartesianIndices((0:0, 1:3)))\n (Armon.Side.Right, CartesianIndices((6:6, 1:3)))\n (Armon.Side.Bottom, CartesianIndices((1:5, 0:0)))\n (Armon.Side.Top, CartesianIndices((1:5, 4:4)))\n\n\n\n\n\n","category":"type"},{"location":"#Armon.@iter_blocks","page":"Home","title":"Armon.@iter_blocks","text":"@iter_blocks for blk in grid\n    # body...\nend\n\nApplies the body of the for-loop in to all blocks of the grid. Threads iterate over the blocks they are assigned to via grid.threads_workload.\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.block_pos_containing_cell","page":"Home","title":"Armon.block_pos_containing_cell","text":"block_pos_containing_cell(grid::BlockGrid, pos::Union{CartesianIndex, NTuple})\n\nReturns two CartesianIndexes: the first is the position of the block containing the cell at pos, the second is the position of the cell in that block.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_origin","page":"Home","title":"Armon.block_origin","text":"block_origin(grid::BlockGrid, pos, include_ghosts=false)\n\nA Tuple of the position of the cell at the bottom left corner of the LocalTaskBlock at pos in the grid.\n\nIf include_ghosts == true, then the cell position includes all ghost cells of the grid.\n\npos can be any of Integer, NTuple{N, Integer} or CartesianIndex.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_at","page":"Home","title":"Armon.block_at","text":"block_at(grid::BlockGrid, idx::CartesianIndex)\n\nThe TaskBlock at position idx in the grid.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_size_at","page":"Home","title":"Armon.block_size_at","text":"block_size_at(grid::BlockGrid, idx)\nblock_size_at(idx, grid_size, static_sized_grid, block_size, remainder_block_size, ghosts)\n\nTheoretical size of block at idx in grid. If ghosts == 0, ghosts cells will be included in the size.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.move_pages-Tuple{BlockGrid}","page":"Home","title":"Armon.move_pages","text":"move_pages(grid::BlockGrid)\n\nMove the pages of all blocks of the grid, including remote blocks, to the NUMA node of the thread which is in charge of working on that block.\n\n\n\n\n\n","category":"method"},{"location":"#Armon.lock_pages-Tuple{BlockGrid}","page":"Home","title":"Armon.lock_pages","text":"lock_pages(grid::BlockGrid)\n\nLocks the pages of all blocks of the grid, including remote blocks.\n\n\n\n\n\n","category":"method"},{"location":"#Block-size-and-iteration","page":"Home","title":"Block size and iteration","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BlockSize\nStaticBSize\nDynamicBSize\nblock_size\nreal_block_size\nghosts\nborder_domain\nghost_domain\nblock_domain_range\nposition\nlin_position\nin_grid\nis_ghost\nBlockRowIterator\nDomainRange","category":"page"},{"location":"#Armon.BlockSize","page":"Home","title":"Armon.BlockSize","text":"BlockSize\n\nDimensions of a LocalTaskBlock.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.StaticBSize","page":"Home","title":"Armon.StaticBSize","text":"StaticBSize{S, Ghost} <: BlockSize\n\nA BlockSize of size S and Ghost cells. S is embedded in the type, this reduces the amount of memory in the parameters of every kernel, as well as allowing the compiler to make some minor optizations in indexing expressions.\n\nGhost cells are included in S: there are S .- 2*Ghost real cells.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.DynamicBSize","page":"Home","title":"Armon.DynamicBSize","text":"DynamicBSize{Ghost} <: BlockSize\n\nSimilar to StaticBSize, but for blocks with a less-than-ideal size: block size is therefore not stored in the type. This results in less compilation when testing for different domain sizes with a constant StaticBSize.\n\nThe number of Ghost cells is still embedded in the type, as it can simplify some indexing expressions and for coherency.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.block_size","page":"Home","title":"Armon.block_size","text":"NTuple of the dimensions of a BlockSize\n\n\n\n\n\n","category":"function"},{"location":"#Armon.real_block_size","page":"Home","title":"Armon.real_block_size","text":"NTuple of the dimensions of a BlockSize, excluding ghost cells\n\n\n\n\n\n","category":"function"},{"location":"#Armon.ghosts","page":"Home","title":"Armon.ghosts","text":"Number of ghost cells of a BlockSize\n\n\n\n\n\n","category":"function"},{"location":"#Armon.border_domain","page":"Home","title":"Armon.border_domain","text":"border_domain(bsize::BlockSize, side::Side.T; single_strip=true)\n\nDomainRange of the real cells along side.\n\nIf single_strip == true,  it includes only one \"strip\" of cells, that is length(border_domain(bsize, side)) == size_along(bsize, side). Otherwise, there are ghosts(bsize) strips of cells: all real cells which would be exchanged with another block along side.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.ghost_domain","page":"Home","title":"Armon.ghost_domain","text":"ghost_domain(bsize::BlockSize, side::Side.T; single_strip=true)\n\nDomainRange of all ghosts cells of side, excluding the corners of the block.\n\nIf single_strip == true, then the domain is only 1 cell thick, positionned at the furthest ghost cell from the real cells. Otherwise, there are ghosts(bsize) strips of cells: all ghost cells which would be exchanged with another block along side.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_domain_range","page":"Home","title":"Armon.block_domain_range","text":"block_domain_range(bsize::BlockSize, corners)\nblock_domain_range(bsize::BlockSize, bottom_left::Tuple, top_right::Tuple)\n\nA DomainRange built from offsets from the corners of bsize.\n\nblock_domain_range(bsize, (0, 0), (0, 0)) is the domain of all real cells in the block. block_domain_range(bsize, (-g, -g), (g, g)) would be the domain of all cells (real cells + g ghost cells) in the block.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.position","page":"Home","title":"Armon.position","text":"position(bsize::BlockSize, i)\n\nN-dim position of the i-th cell in the block.\n\nIf 1 ≤ position(bsize, i)[d] ≤ block_size(bsize)[d] then the cell is not a ghost cell along the d dimension. See is_ghost.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.lin_position","page":"Home","title":"Armon.lin_position","text":"lin_position(bsize::BlockSize, I)\n\nFrom the NTuple (e.g. returned from position), return the linear index in the block. lin_position(bsize, position(bsize, i)) == i.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.in_grid","page":"Home","title":"Armon.in_grid","text":"in_grid(idx, size)\nin_grid(start, idx, size)\n\ntrue if each axis of idx is between start and size. start defaults to 1.\n\nArgument types can any mix of Integer, Tuple or CartesianIndex.\n\njulia> in_grid(1, (1, 2), 2)  # same as `in_grid((1, 1), (1, 2), (2, 2))`\ntrue\n\njulia> in_grid((3, 1), (3, 2))\ntrue\n\njulia> in_grid((1, 3), (3, 2))\nfalse\n\n\n\n\n\nin_grid(idx, grid, axis::Axis.T)\nin_grid(start, idx, grid, axis::Axis.T)\n\nSame as in_grid(start, idx, grid), but only checks along axis.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.is_ghost","page":"Home","title":"Armon.is_ghost","text":"is_ghost(bsize::BlockSize, i, o=0)\n\ntrue if the i-th cell of the block is a ghost cell, false otherwise.\n\no would be a \"ring\" index: o == 1 excludes the first ring of ghost cells, etc.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.BlockRowIterator","page":"Home","title":"Armon.BlockRowIterator","text":"BlockRowIterator(grid::BlockGrid; kwargs...)\nBlockRowIterator(grid::BlockGrid, blk::LocalTaskBlock; kwargs...)\nBlockRowIterator(grid::BlockGrid, blk₁_pos, blk₂_pos; kwargs...)\nBlockRowIterator(grid::BlockGrid, sub_domain::NTuple{2, CartesianIndex}; kwargs...)\nBlockRowIterator(grid::BlockGrid, row_iter::CartesianIndices; global_ghosts=false, all_ghosts=false)\n\nIterate the rows of all blocks of the grid, row by row (and not block by block). This allows to iterate the cells of the grid as if it was a single block.\n\nElements are tuples of (block, global_row_idx, row_range). row_range is the range of cells in block for the current row.\n\nGiving blk will return an iterator on the rows of the block.\n\nGiving blk₁_pos and blk₂_pos will return an iterator over all rows between those blocks.\n\nGiving sub_domain will return an iterator including only the cells contained in sub_domain. sub_domain is a cuboid defined by the position of the cells in the whole domain of grid, using its lower and upper corners.\n\nrow_iter is a iterator over global row indices.\n\nIf global_ghosts == true, then the ghost cells of at the border of the global domain are also returned. If all_ghosts == true, then the ghost cells of at the border of all blocks are also returned.\n\njulia> params = ArmonParameters(; N=(24, 8), nghost=4, block_size=(20, 12), use_MPI=false);\n\njulia> grid = BlockGrid(params);\n\njulia> for (blk, row_idx, row_range) in Armon.BlockRowIterator(grid)\n           println(Tuple(blk.pos), \" - \", row_idx, \" - \", row_range)\n       end\n(1, 1) - (1, 1) - 85:96\n(2, 1) - (2, 1) - 85:96\n(1, 1) - (1, 2) - 105:116\n(2, 1) - (2, 2) - 105:116\n(1, 1) - (1, 3) - 125:136\n(2, 1) - (2, 3) - 125:136\n(1, 1) - (1, 4) - 145:156\n(2, 1) - (2, 4) - 145:156\n(1, 2) - (1, 5) - 85:96\n(2, 2) - (2, 5) - 85:96\n(1, 2) - (1, 6) - 105:116\n(2, 2) - (2, 6) - 105:116\n(1, 2) - (1, 7) - 125:136\n(2, 2) - (2, 7) - 125:136\n(1, 2) - (1, 8) - 145:156\n(2, 2) - (2, 8) - 145:156\n\n\n\n\n\n","category":"type"},{"location":"#Armon.DomainRange","page":"Home","title":"Armon.DomainRange","text":"DomainRange\n\nTwo dimensional range to index a 2D array stored with contiguous rows. Not equivalent to CartesianIndices as it handles StepRanges properly.\n\n\n\n\n\n","category":"type"},{"location":"#Block-states","page":"Home","title":"Block states","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"SolverState\nfirst_state\nSolverStep\nblock_state_machine","category":"page"},{"location":"#Armon.SolverState","page":"Home","title":"Armon.SolverState","text":"SolverState\n\nObject containing all non-constant parameters needed to run the solver, as well as type-parameters needed to avoid runtime dispatch.\n\nThis object is local to a block (or set of blocks): multiple blocks could be at different steps of the solver at once.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.first_state","page":"Home","title":"Armon.first_state","text":"first_state(grid::BlockGrid)\n\nA SolverState which can be used as a global state when outside of a solver cycle. It belongs to the first device block.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.SolverStep","page":"Home","title":"Armon.SolverStep","text":"SolverStep\n\nEnumeration of each state a LocalTaskBlock can be in. block_state_machine advances this state.\n\n\n\n\n\n","category":"module"},{"location":"#Armon.block_state_machine","page":"Home","title":"Armon.block_state_machine","text":"block_state_machine(params::ArmonParameters, blk::LocalTaskBlock)\n\nAdvances the SolverStep state of the blk, apply each step of the solver on the blk. This continues until the current cycle is done, or the block needs to wait for another block to do the ghost cells exchange (block_ghost_exchange) or compute the its time step (next_time_step).\n\nReturns true if we reached the end of the current cycle for blk.\n\n\n\n\n\n","category":"function"},{"location":"#Time-step-reduction","page":"Home","title":"Time step reduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"next_time_step\nGlobalTimeStep\nTimeStepState.WaitingForMPI\nTimeStepState.Done\nTimeStepState.Ready\nTimeStepState.DoingMPI\nTimeStepState.AllContributed","category":"page"},{"location":"#Armon.next_time_step","page":"Home","title":"Armon.next_time_step","text":"next_time_step(params::ArmonParameters, state::SolverState, blk::LocalTaskBlock; already_contributed=false)\nnext_time_step(params::ArmonParameters, state::SolverState, grid::BlockGrid)\n\nCompute the time step of the next cycle. This is done at the start of the current cycle.\n\nSince the current cycle does not rely on an up-to-date time step, the time step reduction is done fully asynchronously, including the global MPI reduction. The accuracy cost of this optimisation is minimal, as the CFL condition prevents the time step from being too large. Additionally, we prevent the time step from increasing of more than +5% of the previous one.\n\nFor first cycle, if no initial time step is given, the time step of the next cycle is reused for the initial cycle.\n\nIf blk is given, its contribution is only added to the state.global_dt (the GlobalTimeStep). Passing the whole block grid will block until the new time step is computed.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.GlobalTimeStep","page":"Home","title":"Armon.GlobalTimeStep","text":"GlobalTimeStep\n\nHolds all information about the current time and time step for the current solver cycle. This struct is global and shared among all blocks.\n\nWhen reaching next_time_step, blocks will contribute to the calculation of the next time step. The last block doing so will start the MPI reduction. The first block reaching the start of the next cycle will wait until this reduction is completed, updating the GlobalTimeStep when so.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.TimeStepState.WaitingForMPI","page":"Home","title":"Armon.TimeStepState.WaitingForMPI","text":"One thread is waiting for the MPI reduction to complete\n\n\n\n\n\n","category":"constant"},{"location":"#Armon.TimeStepState.Done","page":"Home","title":"Armon.TimeStepState.Done","text":"MPI is done: current_dt is up-to-date\n\n\n\n\n\n","category":"constant"},{"location":"#Armon.TimeStepState.Ready","page":"Home","title":"Armon.TimeStepState.Ready","text":"current_dt is up-to-date and blocks can contribute to next_dt\n\n\n\n\n\n","category":"constant"},{"location":"#Armon.TimeStepState.DoingMPI","page":"Home","title":"Armon.TimeStepState.DoingMPI","text":"The MPI reduction has started\n\n\n\n\n\n","category":"constant"},{"location":"#Armon.TimeStepState.AllContributed","page":"Home","title":"Armon.TimeStepState.AllContributed","text":"All blocks contributed to next_dt, one thread will start the MPI reduction\n\n\n\n\n\n","category":"constant"},{"location":"#Block-exchanges","page":"Home","title":"Block exchanges","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"block_ghost_exchange\nstart_exchange\nfinish_exchange\nBlockInterface\nmark_ready_for_exchange!\nexchange_done!\nside_is_done!\nis_side_done\nBlockExchangeState\nBlockExchangeState.NotReady\nBlockExchangeState.InProgress\nBlockExchangeState.Done","category":"page"},{"location":"#Armon.block_ghost_exchange","page":"Home","title":"Armon.block_ghost_exchange","text":"block_ghost_exchange(params::ArmonParameters, state::SolverState, blk::LocalTaskBlock)\n\nHandles communications between the blk neighbours, along the current state.axis. If blk is on one of the edges of the grid, a remote exchange is performed with the neighbouring RemoteTaskBlock, or the global boundary conditions are applied.\n\nReturns true if exchanges were not completed, and the block is waiting on another to be ready for the exchange.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.start_exchange","page":"Home","title":"Armon.start_exchange","text":"start_exchange(\n    params::ArmonParameters,\n    blk::LocalTaskBlock{D, H}, other_blk::RemoteTaskBlock{B}, side::Side.T\n) where {D, H, B}\n\nStart the exchange between one local block and a remote block from another sub-domain. Returns true if the exchange is BlockExchangeState.Done, false if BlockExchangeState.InProgress.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.finish_exchange","page":"Home","title":"Armon.finish_exchange","text":"finish_exchange(\n    params::ArmonParameters,\n    blk::LocalTaskBlock{D, H}, other_blk::RemoteTaskBlock{B}, side::Side.T\n) where {D, H, B}\n\nFinish the exchange between one local block and a remote block from another sub-domain, if MPI communications are done. Returns true if the exchange is BlockExchangeState.Done, false if BlockExchangeState.InProgress.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.BlockInterface","page":"Home","title":"Armon.BlockInterface","text":"BlockInterface\n\nRepresents the interface between two neighbouring TaskBlocks. It synchronizes the state of the blocks to make sure the halo exchange happens when both blocks are ready, and that the operation is done by only one of the blocks.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.mark_ready_for_exchange!","page":"Home","title":"Armon.mark_ready_for_exchange!","text":"mark_ready_for_exchange!(blk, side)\n\nMark blk in the interface along its side as ready for an exchange.\n\nReturn true if the blk should do the exchange, and the new [BlockExchangeState] of the interface.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.exchange_done!","page":"Home","title":"Armon.exchange_done!","text":"exchange_done!(blk, side)\n\nMark the exchange of the interface of the blk along side as done. The other block need to acknowledge this before the exchange state can be reset.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.side_is_done!","page":"Home","title":"Armon.side_is_done!","text":"side_is_done!(blk::LocalTaskBlock, side::Side.T, done::Bool=true)\n\nSet the interface of blk along side as done. This is a non-atomic operation, meant only to avoid repeating the exchange logic multiple times.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.is_side_done","page":"Home","title":"Armon.is_side_done","text":"is_side_done(blk::LocalTaskBlock, side::Side.T)\n\nThe value set by side_is_done!.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.BlockExchangeState","page":"Home","title":"Armon.BlockExchangeState","text":"BlockExchangeState\n\nState of an interface between two blocks, controlling if a cell exchange can happen or did happen.\n\n\n\n\n\n","category":"module"},{"location":"#Armon.BlockExchangeState.NotReady","page":"Home","title":"Armon.BlockExchangeState.NotReady","text":"One of the blocks is not ready yet\n\n\n\n\n\n","category":"constant"},{"location":"#Armon.BlockExchangeState.InProgress","page":"Home","title":"Armon.BlockExchangeState.InProgress","text":"One of the blocks is performing the exchange\n\n\n\n\n\n","category":"constant"},{"location":"#Armon.BlockExchangeState.Done","page":"Home","title":"Armon.BlockExchangeState.Done","text":"The exchange is done\n\n\n\n\n\n","category":"constant"},{"location":"#Block-distribution","page":"Home","title":"Block distribution","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"thread_workload_distribution\nsimple_workload_distribution\nscotch_grid_partition\nblock_grid_from_workload","category":"page"},{"location":"#Armon.thread_workload_distribution","page":"Home","title":"Armon.thread_workload_distribution","text":"thread_workload_distribution(params::ArmonParameters; threads=nothing)\nthread_workload_distribution(\n    threads::Int, grid_size::Tuple;\n    scotch=true, simple=false, diffuse=true, perimeter_first=false, kwargs...\n)\n\nDistribute each block in grid_size among the threads, as evenly as possible.\n\nWith simple == true, blocks are distributed with simple_workload_distribution.\n\nWith scotch == true, the Scotch graph partitioning solver is used to better split the grid. kwargs are passed to scotch_grid_partition.\n\nIf perimeter_first == true, the resulting distribution will have blocks sorted in way that will place neighbours of other threads' blocks first in the list. By doing so, communications between threads may be overlapped more frequently.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.simple_workload_distribution","page":"Home","title":"Armon.simple_workload_distribution","text":"simple_workload_distribution(threads, grid_size)\n\nBasic distribution of B = prod(grid_size) blocks into groups of B ÷ threads, with the remaining blocks distributed evenly.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.scotch_grid_partition","page":"Home","title":"Armon.scotch_grid_partition","text":"scotch_grid_partition(\n    threads, grid_size;\n    strategy=:default, workload_tolerance=0, repart=false, retries=10, weighted=false,\n    static_sized_grid=nothing, block_size=nothing, remainder_block_size=nothing, ghosts=0\n)\n\nSplit grid_size to the threads.\n\nstrategy is passed to Scotch.strat_flags.\n\nworkload_tolerance is the acceptable workload uneveness among the partitions. Giving a few blocks of margin (e.g. at least 1/prod(grid_size)) is preferrable.\n\nweighted == true will distribute blocks while taking into account the number of real cells they have. In this case all parameters of the grid must be present: static_sized_grid, block_size, remainder_block_size and ghosts must be given (obtained with e.g. grid_dimensions).\n\nThe partitioning is random, hence results may vary. To counterbalance this, giving retries > 0 will repeat the partitioning retries times and keep the best one.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.block_grid_from_workload","page":"Home","title":"Armon.block_grid_from_workload","text":"block_grid_from_workload(grid_size, threads_workload)\n\nConvenience function to convert a threads_workload (result of thread_workload_distribution) into an Array of grid_size, with each element assigned to the tid given by the distribution.\n\nThis makes it easy to visualize the efficiency of the distribution.\n\n\n\n\n\n","category":"function"},{"location":"#Device-and-backends","page":"Home","title":"Device and backends","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CPU_HP\ncreate_device\ninit_backend\ndevice_memory_info\nmemory_info\nmemory_required","category":"page"},{"location":"#Armon.CPU_HP","page":"Home","title":"Armon.CPU_HP","text":"CPU_HP\n\nDevice tag for the high-performance CPU backend using multithreading (with Polyester.jl) and vectorisation.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.create_device","page":"Home","title":"Armon.create_device","text":"create_device(::Val{:device_name})\n\nCreate a device object from its name.\n\nDefault devices:\n\n:CPU: the CPU backend of KernelAbstractions.jl\n:CPU_HP: Polyester.jl multithreading\n\nExtensions:\n\n:Kokkos: the default Kokkos.jl device\n:CUDA: the CUDA.jl backend of KernelAbstractions.jl\n:ROCM: the AMDGPU.jl backend of KernelAbstractions.jl\n\n\n\n\n\n","category":"function"},{"location":"#Armon.init_backend","page":"Home","title":"Armon.init_backend","text":"init_backend(params::ArmonParameters, ::Dev; options...)\n\nInitialize the backend corresponding to the Dev device returned by create_device using options. Set the params.backend_options field.\n\nIt must return options, with the backend-specific options removed.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.device_memory_info","page":"Home","title":"Armon.device_memory_info","text":"device_memory_info(device)\n\nThe total and free memory on the device, in bytes.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.memory_info","page":"Home","title":"Armon.memory_info","text":"memory_info(params)\n\nThe total and free memory the current process can store on the params.device.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.memory_required","page":"Home","title":"Armon.memory_required","text":"memory_required(params::ArmonParameters)\n\n(device_memory, host_memory) required for params.\n\nMPI buffers size are included in the appropriate field depending on params.gpu_aware. params.use_MPI and params.neighbours is taken into account.\n\nIf device_is_host, then, device_memory only includes memory required by data arrays and MPI buffers.\n\n\n\n\n\nmemory_required(N::NTuple{2, Int}, block_size::NTuple{2, Int}, ghost::Int, data_type)\nmemory_required(N::NTuple{2, Int}, block_size::NTuple{2, Int}, ghost::Int,\n    device_array_type, host_array_type, buffer_array_type[, solver_state_type])\n\nCompute the number of bytes needed to allocate all blocks. If only data_type is given, then device_array_type, host_array_type and buffer_array_type default to Vector{T}. solver_state_type defaults to SolverState{T, #= default schemes and test =#}.\n\nIn order of returned values:\n\nAmount of bytes needed for all arrays on the device. This amount is also required on the host when the host and device are not the same.\nAmount of bytes needed for all MPI buffers, if the sub-domain has neighbours on all of its sides. If params.gpu_aware, then this memory is allocated on the device.\nAmount of bytes needed on the host memory for all block objects, excluding array data and buffers. This memory is always allocated on the host.\n\nres = memory_required((1000, 1000), (64, 64), 4, CuArray{Float64}, Vector{Float64}, Vector{Float64})\ndevice_memory = res[1]\nhost_memory = res[3] + (device_is_host ? device_memory : res[1])\nif params.gpu_aware\n    device_memory += res[2]\nelse\n    host_memory += res[2]\nend\n\n\n\n\n\n","category":"function"},{"location":"#Kernels","page":"Home","title":"Kernels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"@generic_kernel\n@kernel_init\n@kernel_options\n@index_1D_lin\n@index_2D_lin\n@iter_idx\n@simd_loop\n@simd_threaded_iter\n@simd_threaded_loop\n@threaded\n@threads","category":"page"},{"location":"#Armon.@generic_kernel","page":"Home","title":"Armon.@generic_kernel","text":"@generic_kernel(function definition)\n\nTransforms a single kernel function into six different functions:\n\n4 which run on the CPU using Polyester.jl's multi-threading or not, as well as SIMD or not.\none which uses KernelAbstractions.jl to make a GPU-compatible kernel\na main function, which will take care of calling the two others depending if we want to use the GPU or not.\n\nTo do this, two things are done:\n\nAll calls to @index_1D_lin(), @index_2D_lin() and @iter_idx() are replaced by their equivalent in their respective platforms: a simple loop index for CPUs, and a call to KA.jl's @index for GPUs.\nArguments to each function are edited \n\nA kernel function must call one of @index_1D_lin() or @index_2D_lin() at least once, since this  will determine which type of indexing to use as well as which parameters to add.\n\nThe indexing macro @iter_idx gives the linear index to the current iteration (on CPU) or global  thread (on GPU).\n\nThis means that in order to call the new main function, one needs to take into account which indexing macro was used:\n\nIn all cases, params::ArmonParameters is the first argument\nThen, depending on the indexing macro used:\n@index_1D_lin() : loop_range::OrdinalRange{Int}\n@index_2D_lin() : main_range::OrdinalRange{Int}, inner_range::OrdinalRange{Int}\nAn optional keyword argument, no_threading, allows to override the use_threading parameter, which can be useful in asynchronous contexts. It defaults to false.\n\nUsing KA.jl's @Const to annotate arguments is supported, but they will be present only in the GPU kernel definition.\n\nFurther customisation of the kernel and main function can be obtained using @kernel_options and @kernel_init.\n\n@generic_kernel f_kernel(A)\n    i = @index_1D_lin()\n    A[i] += 1\nend\n\nparams.use_gpu = false\nf(params, 1:10)  # CPU call\n\nparams.use_gpu = true\nf(params, 1:10)  # GPU call\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@kernel_init","page":"Home","title":"Armon.@kernel_init","text":"@kernel_init(expr)\n\nAllows to initialize some internal variables of the kernel before the loop. The given expression must NOT depend on any index.  You must not use any indexing macro (@index_1D_lin(), etc...) in the expression.\n\nAll paramerters of the kernel are available during the execution of the init expression. On GPU, this expression will be executed by each thread.\n\nThis is a workaround for a limitation of Polyester.jl which prevents you from typing variables.\n\n@generic_kernel function simple_kernel(a, b)\n    @kernel_init begin\n        c::Float64 = sin(0.123)\n    end\n    i = @index_1D_lin()\n    a[i] += b[i] * c\nend\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@kernel_options","page":"Home","title":"Armon.@kernel_options","text":"@kernel_options(options...)\n\nMust be used (once, and explicitly) in the definition of a @generic_kernel function.\n\nGives options for @generic_kernel to adjust the resulting functions.\n\nThe possible options are:\n\ndebug:      Prints the generated functions to stdout at compile time.\n\n@generic_kernel function add_kernel(a, b)\n    @kernel_options(debug)\n    i = @index_1D_lin()\n    a[i] += b[i]\nend\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@index_1D_lin","page":"Home","title":"Armon.@index_1D_lin","text":"@index_1D_lin()\n\nIndexing macro to use in a @generic_kernel function.  Returns a linear index to access the 1D arrays used by the kernel.\n\nCannot be used alongside @index_2D_lin().\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@index_2D_lin","page":"Home","title":"Armon.@index_2D_lin","text":"@index_2D_lin()\n\nIndexing macro to use in a @generic_kernel function.  Returns a linear index to access the 2D arrays used by the kernel.\n\nCannot be used alongside @index_1D_lin().\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@iter_idx","page":"Home","title":"Armon.@iter_idx","text":"@iter_idx()\n\nIndexing macro to use in a @generic_kernel function.  Returns a linear index to access the 2D arrays used by the kernel.\n\nEquivalent to KernelAbstractions.jl's @index(Global, Linear).\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@simd_loop","page":"Home","title":"Armon.@simd_loop","text":"@simd_loop(expr)\n\nAllows to enable/disable SIMD optimisations for a loop. When SIMD is enabled, it is assumed that there is no dependencies between each iterations of the loop.\n\n    @simd_loop for i = 1:n\n        y[i] = x[i] * (x[i-1])\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@simd_threaded_iter","page":"Home","title":"Armon.@simd_threaded_iter","text":"@simd_threaded_iter(range, expr)\n\nSame as @simd_threaded_loop(expr), but instead of slicing the range of the for loop in expr, we slice the range given as the first parameter and distribute the slices evenly to the threads.\n\nThe inner @simd loop assumes there is no dependencies between each iteration.\n\n    @simd_threaded_iter 4:2:100 for i in 1:100\n        y[i] = log10(x[i]) + x[i]\n    end\n    # is equivalent to (without threading and SIMD)\n    for j in 4:2:100\n        for i in (1:100) .+ (j - 1)\n            y[i] = log10(x[i]) + x[i]\n        end\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@simd_threaded_loop","page":"Home","title":"Armon.@simd_threaded_loop","text":"@simd_threaded_loop(expr)\n\nAllows to enable/disable multithreading and/or SIMD of the loop depending on the parameters. When using SIMD, @fastmath and @inbounds are used.\n\nIn order to use SIMD and multithreading at the same time, the range of the loop is split in even  batches. Each batch has a size of params.simd_batch iterations, meaning that the inner @simd loop has a fixed number of iterations, while the outer threaded loop will have N ÷ params.simd_batch iterations.\n\nThe loop range is assumed to be increasing, i.e. this is correct: 1:2:100, this is not: 100:-2:1 The inner @simd loop assumes there is no dependencies between each iteration.\n\n    @simd_threaded_loop for i = 1:n\n        y[i] = log10(x[i]) + x[i]\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@threaded","page":"Home","title":"Armon.@threaded","text":"@threaded(expr)\n\nAllows to enable/disable multithreading of the loop depending on the parameters.\n\nThe default condition is params.use_threading && !params.use_cache_blocking. By passing :outside_kernel before expr, the condition becomes simply params.use_threading.\n\n    @threaded for i = 1:n\n        y[i] = log10(x[i]) + x[i]\n    end\n\n\n\n\n\n","category":"macro"},{"location":"#Armon.@threads","page":"Home","title":"Armon.@threads","text":"Controls which multi-threading library to use.\n\n\n\n\n\n","category":"macro"},{"location":"#Logging","page":"Home","title":"Logging","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BlockLogEvent\ncollect_logs\nanalyse_log_stats\nBlockGridLogStats\nBLOCK_LOG_THREAD_LOCAL_STORAGE","category":"page"},{"location":"#Armon.BlockLogEvent","page":"Home","title":"Armon.BlockLogEvent","text":"BlockLogEvent\n\nInfo about a block, emitted after a call to block_state_machine which successfully advanced the internal state of the block, only if params.log_blocks == true.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.collect_logs","page":"Home","title":"Armon.collect_logs","text":"collect_logs(grid::BlockGrid)\n\nCollect all BlockLogEvent of the grid into a single object (a BlockGridLog). See analyse_log_stats to print metrics about all blocks.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.analyse_log_stats","page":"Home","title":"Armon.analyse_log_stats","text":"analyse_log_stats(f, grid_log::BlockGridLog)\n\nCall f for each block of the grid_log, passing the block's position (a CartesianIndex), a Vector of BlockLogEvents, and the size of the block.\n\n\n\n\n\nanalyse_log_stats(grid_log::BlockGridLog)\n\nCrunch all data of grid_log into tangible metrics contained in a BlockGridLogStats.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.BlockGridLogStats","page":"Home","title":"Armon.BlockGridLogStats","text":"BlockGridLogStats\n\nMetrics about the blocks of a full solver excution.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.BLOCK_LOG_THREAD_LOCAL_STORAGE","page":"Home","title":"Armon.BLOCK_LOG_THREAD_LOCAL_STORAGE","text":"BLOCK_LOG_THREAD_LOCAL_STORAGE::Dict{UInt16, Int32}\n\nIncremented by 1 every time a BlockLogEvent is created in a thread, i.e. each time a block has solver kernels applied to it through block_state_machine.\n\nSince only differences between values are interesting, no need to reset it.\n\n\n\n\n\n","category":"constant"},{"location":"#Utility","page":"Home","title":"Utility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Axis\nSide\nSolverException\n@section","category":"page"},{"location":"#Armon.Axis","page":"Home","title":"Armon.Axis","text":"Axis\n\nEnumeration of the axes of the domain\n\n\n\n\n\n","category":"module"},{"location":"#Armon.Side","page":"Home","title":"Armon.Side","text":"Side\n\nEnumeration of the sides of the domain\n\n\n\n\n\n","category":"module"},{"location":"#Armon.SolverException","page":"Home","title":"Armon.SolverException","text":"SolverException\n\nThrown when the solver encounters an invalid state.\n\nThe category field can be used to distinguish between error types without inspecting the error message:\n\n:config: a problem in the solver configuration, usually thrown when constructing ArmonParameters\n:cpp: a C++ exception thrown by the C++ Kokkos backend\n:time: an invalid time step\n\nErrorExceptions thrown by the solver represent internal errors.\n\n\n\n\n\n","category":"type"},{"location":"#Armon.@section","page":"Home","title":"Armon.@section","text":"@section(name, expr)\n@section(name, options, expr)\n\nIntroduce a profiling section around expr. Sections can be nested. Sections do not introduce a new scope.\n\nPlaced before a for-loop, a new section will be started for each iteration. name can interpolate using loop variables (like Test.@testset).\n\nIt is assumed that a params variable of ArmonParameters is present in the scope of the @section.\n\noptions is of the form key=value:\n\nasync (default: false): if async=false (and !params.time_async), a barrier (wait(params)) is added at the end of the section.\n\nparams = ArmonParameters(#= ... =#)\n\n@section \"Iteration $i\" for i in 1:10\n    j = @section \"Foo\" begin\n        foo(i)\n    end\n\n    @section \"Some calculation\" begin\n        k = bar(i, j)\n    end\n\n    @sync begin\n        @async begin\n            @section \"Task 1\" async=true my_task_1(i, j, k)\n        end\n\n        @async begin\n            @section \"Task 2\" async=true my_task_2(i, j, k)\n        end\n    end\nend\n\n\n\n\n\n","category":"macro"},{"location":"#NUMA-utilities","page":"Home","title":"NUMA utilities","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"array_pages\ntouch_pages\nmove_pages(::Vector{Ptr{T}}, ::Any) where T\nlock_pages(::Ptr, ::Any)\nunlock_pages","category":"page"},{"location":"#Armon.array_pages","page":"Home","title":"Armon.array_pages","text":"array_pages(A::Ptr, A_length)\narray_pages(A::DenseArray)\n\nIterator over the memory pages (aligned to PAGE_SIZE) used by the array A.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.touch_pages","page":"Home","title":"Armon.touch_pages","text":"touch_pages(pages)\n\nParses over each of pages and \"touch\" them by loading a value then storing it again.\n\nThe (only) use case is to manually trigger the first-touch policy and/or force the kernel to physically allocate all pages.\n\n\n\n\n\n","category":"function"},{"location":"#Armon.move_pages-Union{Tuple{T}, Tuple{Array{Ptr{T}, 1}, Any}} where T","page":"Home","title":"Armon.move_pages","text":"move_pages(pages::Vector{Ptr}, target_node; retries=4)\nmove_pages(pages::OrdinalRange{Ptr, Int}, target_node)\nmove_pages(A::DenseArray, target_node)\n\nMove all pages (of either the array A or from the pages iterable) to the target_node (1-index).\n\nThe move status of all pages is checked afterwards. No error is thrown only if all pages are on target_node at the end of the move.\n\nIn case some pages cannot be moved because of EBUSY status, the move is retried retries-times until it succeeds, with a short sleep in-between attempts.\n\n\n\n\n\n","category":"method"},{"location":"#Armon.lock_pages-Tuple{Ptr, Any}","page":"Home","title":"Armon.lock_pages","text":"lock_pages(ptr, len)\nlock_pages(pages::OrdinalRange{Ptr, Int})\nlock_pages(A::DenseArray)\n\nLock the pages ranging from ptr to ptr+len on the RAM, preventing the kernel from moving it around or putting it in swap memory.\n\n\n\n\n\n","category":"method"},{"location":"#Armon.unlock_pages","page":"Home","title":"Armon.unlock_pages","text":"unlock_pages(ptr, len)\nunlock_pages(pages::OrdinalRange{Ptr, Int})\nunlock_pages(A::DenseArray)\n\nUnlock the pages, opposite of lock_pages.\n\n\n\n\n\n","category":"function"}]
}
